run_name: 30e
run_folder: TransformerCFM

method: GenerativeUnfolding
process_params:
  train_slice: [0., 0.6]
  val_slice: [0.6, 0.65]
  test_slice: [0.65, 1.0]
  training_file: "data/Pythia26_full.npz"
  analysis_file: "data/Herwig_full.npz"

# Preprocessing
hard_preprocessing:
  type:
  unit_hypercube: False
  args:
    pt_conserved: True
reco_preprocessing:
  type:

# Training
lr: 1.e-3
batch_size: 128
batch_size_sample: 10000
lr_scheduler: cosine_annealing
weight_decay: 0.
betas: [0.9, 0.99]
epochs: 30

# Architecture
model: CFMwithTransformer
bayesian: False

latent_space: gaussian
uniform_channels: [4]
uniform_bounds: [0., 1.]
minimum_noise_scale: 0
t_noise_scale: 0


dropout: 0.0
dim_embedding: 16
n_head: 2
n_encoder_layers: 2
n_decoder_layers: 2
dim_feedforward: 32
embedding_nets: False

network_params:
  network_class: Subnet
  internal_size: 76
  layers_per_block: 4
  activation: SiLU

  embed_t: False
  embed_t_mode: gfprojection
  embed_t_dim: 16

solver: ODE
solver_params:
  t_min: 0
  t_max: 1
  mixed_precision: False
  sde_method: srk
  ode_method: dopri5
  atol: 1.e-5
  rtol: 1.e-5
  step_size: 1.e-2

evaluate_train: False
evaluate_analysis: False